---
layout: post
title: "基于scrapy-redis分布式网络爬虫存储数据分析（一）"
date: 2016-09-01 15:23:18 +0800
categories: scrapy-redis
tags: 爬虫 数据分析 数据存储
---
* content
{:toc}


# 基本设置 #

## 配置环境 ##
- **Python：** Python 2.7.11 (v2.7.11:6d1b6a68f775, Dec  5 2015, 20:32:19) [MSC v.1500 32 bit (Intel)] on win32
- **Redis：** Redis server v=3.2.100 sha=00000000:0 malloc=jemalloc-3.6.0 bits=64 build=dd26f1f93c5130ee
- **Scrapy：** Scrapy 1.1.1
- **redis-py：** 2.10.5
- **scrapy-redis：** scrapy-redis-0.6.3
- **jieba** jieba-0.38 


开源代码：
https://github.com/fxsjy/jieba


学习笔记：

 - 绪论部分 https://segmentfault.com/a/1190000004061791
 - 分词模式  https://segmentfault.com/a/1190000004065927
 - DAG（有向无环图） https://segmentfault.com/a/1190000004085949
 - 详细使用过程介绍  http://blog.csdn.net/u010454729/article/details/40476483









## 安装 ##
进入到pip.exe目录下，使用安装命令`pip install redis`即可。如果缺少其他组件也可以通过方法`pip install modulename`安装。

![install redis-py](http://upload-images.jianshu.io/upload_images/1242974-a623573e3d3bd6fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 调试 ##
python代码调试
http://www.cnblogs.com/qi09/archive/2012/02/10/2344959.html

# 基本架构 #
Scrapy基于事件驱动网络框架 [Twisted](http://twistedmatrix.com/trac/) 编写。因此，Scrapy基于并发性考虑由非阻塞(即异步)的实现。Scrapy中的数据流由执行引擎控制，其过程如下:


 
![Scrapy架构](http://upload-images.jianshu.io/upload_images/1242974-f1908012ebe3aabe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

1. 引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。
2. 引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。
3. 引擎向调度器请求下一个要爬取的URL。
4. 调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。
5. 一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。
6. 引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。
7. Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。
8. 引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。
9. (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。


# 总结 #

本部分主要介绍爬虫需要配置的基本环境，介绍了使用的scrapy框架的爬取数据的基本流程，爬取具体信息涉及到的知识点主要有以下部分：

- xpath
- post和get的理解
- http状态码
- 正则表达式


